# Implementation Plan: REQ-002A Smart Crawling Engine (Core)

**Status**: ğŸ”„ In Progress
**Started**: 2026-02-13
**Last Updated**: 2026-02-13 (Phase 3 complete)
**Estimated Completion**: 2026-02-20

---

**CRITICAL INSTRUCTIONS**: After completing each phase:
1. Check off completed task checkboxes
2. Run all quality gate validation commands
3. Verify ALL quality gate items pass
4. Update "Last Updated" date above
5. Document learnings in Notes section
6. Only then proceed to next phase

**DO NOT skip quality gates or proceed with failing checks**

---

## Overview

### Feature Description

Playwright ê¸°ë°˜ í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì €ë¡œ ì‹¤ì œ ì‚¬ìš©ìì™€ ë™ì¼í•˜ê²Œ ì›¹ í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ì—¬ í˜ì´ì§€ êµ¬ì¡°, í¼, API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì‹ë³„í•˜ëŠ” ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì—”ì§„. REQ-001(ì •ê·œì‹ í¬ë¡¤ë§)ì´ ì •ì  HTMLë§Œ ë¶„ì„í•˜ëŠ” ë°˜ë©´, REQ-002AëŠ” JavaScript ë Œë”ë§ì´ í•„ìš”í•œ SPA(Single Page Application)ê¹Œì§€ ì§€ì›í•œë‹¤.

### Success Criteria
- [ ] Playwright ê¸°ë°˜ í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì €ë¡œ ì‹¤ì œ ì‚¬ìš©ìì™€ ë™ì¼í•˜ê²Œ í˜ì´ì§€ë¥¼ íƒìƒ‰í•œë‹¤
- [ ] JavaScript ë Œë”ë§ì´ í•„ìš”í•œ SPAë¥¼ ì§€ì›í•œë‹¤
- [ ] í¼ í•„ë“œ, ë²„íŠ¼, ë§í¬, API í˜¸ì¶œì„ ìë™ìœ¼ë¡œ ì‹ë³„í•œë‹¤
- [ ] í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì§€ì‹ ê·¸ë˜í”„ í˜•íƒœë¡œ êµ¬ì¡°í™”í•œë‹¤
- [ ] í¬ë¡¤ë§ ê¹Šì´ ë° ë²”ìœ„ë¥¼ ì‚¬ìš©ìê°€ ì„¤ì •í•  ìˆ˜ ìˆë‹¤
- [ ] robots.txt ë° í¬ë¡¤ë§ ì œí•œ ì •ì±…ì„ ì¤€ìˆ˜í•  ìˆ˜ ìˆëŠ” ì˜µì…˜ì„ ì œê³µí•œë‹¤

### User Impact

ë³´ì•ˆ ë‹´ë‹¹ìê°€ ëŒ€ìƒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ URLë§Œ ì…ë ¥í•˜ë©´, SPAë¥¼ í¬í•¨í•œ ëª¨ë“  í˜ì´ì§€ì™€ API ì—”ë“œí¬ì¸íŠ¸ê°€ ìë™ìœ¼ë¡œ ë§¤í•‘ëœë‹¤. ê¸°ì¡´ ì •ê·œì‹ í¬ë¡¤ëŸ¬ê°€ ë†“ì¹˜ëŠ” JavaScript ë™ì  ì½˜í…ì¸ ì™€ XHR/fetch API í˜¸ì¶œê¹Œì§€ íƒì§€í•˜ì—¬ ì§„ë‹¨ ë²”ìœ„ ëˆ„ë½ì„ ë°©ì§€í•œë‹¤.

---

## Architecture Decisions

| Decision | Rationale | Trade-offs |
|----------|-----------|------------|
| ë³„ë„ `SmartCrawlerEngine` í´ë˜ìŠ¤ (ê¸°ì¡´ `CrawlerEngine` í™•ì¥ ëŒ€ì‹ ) | regex í¬ë¡¤ëŸ¬(httpx)ì™€ smart í¬ë¡¤ëŸ¬(Playwright)ëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ë™ì‘. ê¸°ì¡´ ì—”ì§„ ì•ˆì •ì„± ë³´ì¡´ | ì¼ë¶€ ë¡œì§ ì¤‘ë³µ(BFS, scope check) ê°€ëŠ¥í•˜ë‚˜ compositionìœ¼ë¡œ í•´ê²° |
| `BrowserManager` ë¶„ë¦¬ | Playwright ë¸Œë¼ìš°ì € ìƒëª…ì£¼ê¸°(ì‹œì‘/ì¢…ë£Œ/í˜ì´ì§€ ê´€ë¦¬)ë¥¼ í¬ë¡¤ë§ ë¡œì§ê³¼ ë¶„ë¦¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í™•ë³´ | íŒŒì¼ í•˜ë‚˜ ì¶”ê°€ë˜ì§€ë§Œ SRP ì¤€ìˆ˜ |
| `PageAnalyzer` ë¶„ë¦¬ | ë Œë”ë§ëœ DOM ë¶„ì„ ë¡œì§ì„ ì—”ì§„ì—ì„œ ë¶„ë¦¬. ê¸°ì¡´ `regex_parser`ì™€ ëŒ€ì‘ë˜ëŠ” ì—­í•  | ì¶”ìƒí™” ë ˆì´ì–´ í•˜ë‚˜ ì¶”ê°€ |
| `NetworkInterceptor` ë¶„ë¦¬ | XHR/fetch ìº¡ì²˜ë¥¼ DOM ë¶„ì„ê³¼ ë³„ë„ ê´€ì‹¬ì‚¬ë¡œ ë¶„ë¦¬. ì‹¤íŒ¨ ëª¨ë“œê°€ ë‹¤ë¦„(selector vs timing) | Phase 2Bì—ì„œ ë³„ë„ TDD cycle |
| Knowledge Graphë¥¼ `crawl_types.py`ì— ì •ì˜ | ê¸°ì¡´ ëª¨ë¸ë“¤ê³¼ ì¼ê´€ëœ ìœ„ì¹˜. CrawlResultì—ì„œ ì°¸ì¡° ê°€ëŠ¥ | crawl_types.pyê°€ ì»¤ì§€ë‚˜ í˜„ì¬ ê·œëª¨ì—ì„œ ë¬¸ì œì—†ìŒ |
| `page.route()` ê¸°ë°˜ í…ŒìŠ¤íŠ¸ | ì‹¤ì œ Playwright ë¸Œë¼ìš°ì € ì‚¬ìš©í•˜ë˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ê°€ë¡œì±„ì„œ ì œì–´. respx íŒ¨í„´ê³¼ ìœ ì‚¬ | CIì— ë¸Œë¼ìš°ì € ì„¤ì¹˜ í•„ìš”í•˜ë‚˜ ê°€ì¥ í˜„ì‹¤ì  |
| REQ-002B í™•ì¥ ê³ ë ¤ | PageAnalyzerì— LLM ì£¼ì… ê°€ëŠ¥í•œ êµ¬ì¡° ì„¤ê³„. ì§ì ‘ LLM í˜¸ì¶œì€ ì•ˆ í•¨ | ì•½ê°„ì˜ ì¶”ìƒí™” ì˜¤ë²„í—¤ë“œ |

---

## Dependencies

### Required Before Starting
- [x] REQ-001 ì •ê·œì‹ í¬ë¡¤ë§ ì—”ì§„ ì™„ë£Œ (215 tests, 98% coverage)
- [x] ê¸°ì¡´ ëª¨ë“ˆ ë™ì‘ í™•ì¸: url_resolver, robots_parser, url_pattern, sitemap, exporter

### External Dependencies
- `playwright>=1.40` (ì‹ ê·œ ì¶”ê°€)
- Chromium ë¸Œë¼ìš°ì € ë°”ì´ë„ˆë¦¬ (`playwright install chromium`)

---

## Test Strategy

### Testing Approach
**TDD Principle**: Write tests FIRST, then implement to make them pass

**Playwright Test Guidelines**:
- ALWAYS use `async with` or fixture teardownë¡œ browser/page ì •ë¦¬
- NEVER share browser context between tests (state leakage ë°©ì§€)
- page.wait_for_selector() ì‚¬ìš©, time.sleep() ê¸ˆì§€
- page.on() listenerëŠ” í…ŒìŠ¤íŠ¸ í›„ ë°˜ë“œì‹œ cleanup

### Test Pyramid for This Feature
| Test Type | Coverage Target | Purpose |
|-----------|-----------------|---------|
| **Unit Tests** | >=80% | BrowserManager, PageAnalyzer, NetworkInterceptor, GraphBuilder |
| **Integration Tests** | Critical paths | SmartCrawlerEngine ì „ì²´ ì›Œí¬í”Œë¡œìš°, CLI í†µí•© |
| **E2E Tests** | Key user flows | í¬ë¡¤ë§â†’ê·¸ë˜í”„â†’JSON ë‚´ë³´ë‚´ê¸° ì „ì²´ íŒŒì´í”„ë¼ì¸ |

### Test File Organization
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ test_browser_manager.py     # [NEW] ë¸Œë¼ìš°ì € ìƒëª…ì£¼ê¸° í…ŒìŠ¤íŠ¸
â”‚   â”‚   â”œâ”€â”€ test_page_analyzer.py       # [NEW] DOM ë¶„ì„ í…ŒìŠ¤íŠ¸
â”‚   â”‚   â”œâ”€â”€ test_network_interceptor.py # [NEW] API ìº¡ì²˜ í…ŒìŠ¤íŠ¸
â”‚   â”‚   â””â”€â”€ test_graph_builder.py       # [NEW] ê·¸ë˜í”„ ë³€í™˜ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ test_crawl_types.py         # [MODIFY] ìƒˆ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¶”ê°€
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ crawler/
â”‚       â””â”€â”€ test_smart_engine.py        # [NEW] ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ í†µí•© í…ŒìŠ¤íŠ¸
```

### Coverage Requirements by Phase
- **Phase 1 (Foundation)**: BrowserManager + ëª¨ë¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (>=80%)
- **Phase 2A (DOM Analysis)**: PageAnalyzer ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (>=80%)
- **Phase 2B (Network Capture)**: NetworkInterceptor ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (>=80%)
- **Phase 3 (Engine)**: SmartCrawlerEngine í†µí•© í…ŒìŠ¤íŠ¸ (>=80%)
- **Phase 4 (Graph)**: KnowledgeGraph + GraphBuilder í…ŒìŠ¤íŠ¸ (>=80%)
- **Phase 5 (CLI)**: CLI í†µí•© í…ŒìŠ¤íŠ¸ (>=70%)

### Test Naming Convention
```python
# File: test_{module_name}.py
# Class: Test{ComponentName}
# Function: test_{behavior}_{condition}_{expected_result}
# Example: test_extract_links_from_rendered_dom_returns_absolute_urls
# Pattern: Arrange -> Act -> Assert
```

---

## Implementation Phases

### Phase 1: Foundation - Dependencies & BrowserManager
**Goal**: Playwright ì˜ì¡´ì„± ì„¤ì •, CrawlConfig í™•ì¥, BrowserManager í´ë˜ìŠ¤ êµ¬í˜„
**Estimated Time**: 3 hours
**Status**: âœ… Complete (228 tests, 99% coverage on new code)

#### Tasks

**RED: Write Failing Tests First**

- [x] **Test 1.1**: CrawlConfig ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ í•„ë“œ í™•ì¥ í…ŒìŠ¤íŠ¸ (6 tests)
  - File(s): `tests/unit/models/test_crawl_types.py` (ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€)
  - Expected: Tests FAIL (red) because new fields don't exist
  - Details:
    - `test_crawl_config_headless_default_true` â€” headless ê¸°ë³¸ê°’ True
    - `test_crawl_config_wait_until_default_networkidle` â€” wait_until ê¸°ë³¸ê°’ "networkidle"
    - `test_crawl_config_viewport_width_default_1280` â€” viewport_width ê¸°ë³¸ê°’ 1280
    - `test_crawl_config_viewport_height_default_720` â€” viewport_height ê¸°ë³¸ê°’ 720
    - `test_crawl_config_auto_detect_spa_default_true` â€” SPA ìë™ ê°ì§€ ê¸°ë³¸ í™œì„±
    - `test_crawl_config_smart_fields_backward_compatible` â€” ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•ŠìŒ

- [x] **Test 1.2**: BrowserManager í´ë˜ìŠ¤ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (7 tests)
  - File(s): `tests/unit/crawler/test_browser_manager.py` (ì‹ ê·œ íŒŒì¼)
  - Expected: Tests FAIL (red) because BrowserManager doesn't exist
  - Details:
    - `test_browser_manager_async_context_manager_starts_browser` â€” async with íŒ¨í„´ìœ¼ë¡œ ë¸Œë¼ìš°ì € ì‹œì‘
    - `test_browser_manager_async_context_manager_closes_on_exit` â€” ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ì‹œ ë¸Œë¼ìš°ì € ë‹«í˜
    - `test_browser_manager_closes_browser_on_exception` â€” ì˜ˆì™¸ ì‹œì—ë„ ì •ë¦¬
    - `test_browser_manager_creates_page_with_viewport` â€” í˜ì´ì§€ ìƒì„± + ë·°í¬íŠ¸ ì„¤ì •
    - `test_browser_manager_sets_user_agent` â€” UA ì„¤ì •
    - `test_browser_manager_headless_mode` â€” headless ëª¨ë“œ í™•ì¸
    - `test_browser_manager_reuses_browser_across_pages` â€” ë¸Œë¼ìš°ì € ì¬ì‚¬ìš©, í˜ì´ì§€ë§Œ ìƒˆë¡œ ìƒì„±

**GREEN: Implement to Make Tests Pass**

- [x] **Task 1.3**: ì˜ì¡´ì„± ì¶”ê°€ ë° CrawlConfig í™•ì¥
  - File(s): `pyproject.toml`, `src/eazy/models/crawl_types.py`
  - Goal: Test 1.1 í†µê³¼
  - Details:
    - `pyproject.toml`ì— `playwright>=1.40` ì˜ì¡´ì„± ì¶”ê°€
    - CrawlConfigì— ì¶”ê°€ í•„ë“œ:
      - `headless: bool = True`
      - `wait_until: Literal["networkidle", "domcontentloaded", "load", "commit"] = "networkidle"`
      - `viewport_width: int = 1280`
      - `viewport_height: int = 720`
      - `auto_detect_spa: bool = True`

- [x] **Task 1.4**: BrowserManager í´ë˜ìŠ¤ êµ¬í˜„
  - File(s): `src/eazy/crawler/browser_manager.py` (ì‹ ê·œ íŒŒì¼)
  - Goal: Test 1.2 í†µê³¼
  - Details:
    - async context manager (`__aenter__`, `__aexit__`)
    - `launch()` â€” Chromium ë¸Œë¼ìš°ì € ì‹œì‘
    - `new_page()` â€” ìƒˆ í˜ì´ì§€ ìƒì„± (viewport, UA ì„¤ì •)
    - `close()` â€” ë¸Œë¼ìš°ì € ì¢…ë£Œ
    - CrawlConfigì—ì„œ headless, viewport, user_agent ì½ê¸°

**REFACTOR: Clean Up Code**

- [x] **Task 1.5**: ì½”ë“œ í’ˆì§ˆ ê°œì„ 
  - Files: `src/eazy/crawler/browser_manager.py`, `src/eazy/models/crawl_types.py`
  - Goal: í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ì„œ ì„¤ê³„ ê°œì„ 
  - Checklist:
    - [x] Google ìŠ¤íƒ€ì¼ docstring ì¶”ê°€
    - [x] `__all__` export ë¦¬ìŠ¤íŠ¸ ì •ë¦¬
    - [x] Playwright conftest fixture â€” ë¶ˆí•„ìš” (unittest.mockìœ¼ë¡œ ì¶©ë¶„)
    - [x] ê¸°ì¡´ 215ê°œ í…ŒìŠ¤íŠ¸ ì „ë¶€ í†µê³¼ ì¬í™•ì¸ (228 total)

#### Quality Gate

**STOP: Do NOT proceed to Phase 2A until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [x] **Red Phase**: Tests were written FIRST and initially failed
- [x] **Green Phase**: Production code written to make tests pass
- [x] **Refactor Phase**: Code improved while tests still pass
- [x] **Coverage Check**: BrowserManager 97%, crawl_types 100%

**Build & Tests**:
- [x] **All Tests Pass**: 215 existing + 13 new = 228 total passed
- [x] **No Flaky Tests**: ì¼ê´€ëœ ê²°ê³¼
- [x] **Playwright Install**: `playwright install chromium` ì •ìƒ ë™ì‘

**Code Quality**:
- [x] **Linting**: `uv run ruff check src/ tests/` â€” All checks passed
- [x] **Formatting**: `uv run ruff format --check src/ tests/` â€” 40 files already formatted
- [x] **Type Safety**: ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ ì ìš©

**Validation Commands**:
```bash
# ì˜ì¡´ì„± ì„¤ì¹˜ (ìµœì´ˆ 1íšŒ)
uv sync
playwright install chromium

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run pytest tests/ -v

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
uv run pytest tests/ --cov=src/eazy --cov-report=term-missing

# ë¦°íŒ…/í¬ë§·íŒ…
uv run ruff check src/ tests/
uv run ruff format --check src/ tests/
```

**Manual Test Checklist**:
- [x] `BrowserManager(config)` async with íŒ¨í„´ìœ¼ë¡œ ë¸Œë¼ìš°ì € ì‹œì‘/ì¢…ë£Œ í™•ì¸
- [x] ìƒˆ CrawlConfig í•„ë“œê°€ ê¸°ì¡´ ì½”ë“œì— ì˜í–¥ ì—†ìŒ í™•ì¸ (215 ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì „ë¶€ í†µê³¼)
- [x] Playwright Chromium ì •ìƒ ì„¤ì¹˜ í™•ì¸

---

### Phase 2A: Page Analyzer - DOM Analysis
**Goal**: ë Œë”ë§ëœ DOMì—ì„œ ë§í¬, í¼, ë²„íŠ¼ì„ ì¶”ì¶œí•˜ëŠ” PageAnalyzer í´ë˜ìŠ¤ êµ¬í˜„ + SPA ê°ì§€
**Estimated Time**: 2 hours
**Status**: âœ… Complete (240 tests, 87% coverage on PageAnalyzer)

#### Tasks

**RED: Write Failing Tests First**

- [x] **Test 2A.1**: PageAnalyzer DOM ì¶”ì¶œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
  - File(s): `tests/unit/crawler/test_page_analyzer.py` (ì‹ ê·œ íŒŒì¼)
  - Expected: Tests FAIL (red) because PageAnalyzer doesn't exist
  - Details:
    - `test_extract_links_from_rendered_dom_returns_absolute_urls` â€” <a> íƒœê·¸ì—ì„œ ë§í¬ ì¶”ì¶œ
    - `test_extract_links_skips_javascript_href` â€” `href="javascript:void(0)"` ë¬´ì‹œ
    - `test_extract_links_skips_anchor_only_href` â€” `href="#section"` ë¬´ì‹œ
    - `test_extract_forms_from_rendered_page` â€” <form> í•„ë“œ ì¶”ì¶œ (action, method, inputs)
    - `test_extract_forms_from_spa_dynamic_content` â€” JS ë Œë”ë§ í›„ ë™ì  í¼ ì¶”ì¶œ
    - `test_extract_buttons_with_type_submit` â€” submit ë²„íŠ¼ ì¶”ì¶œ
    - `test_extract_buttons_with_onclick` â€” onclick í•¸ë“¤ëŸ¬ ìˆëŠ” ë²„íŠ¼ ì¶”ì¶œ
    - `test_handle_empty_page_returns_empty_results` â€” ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬
    - `test_extract_page_title_from_rendered_dom` â€” <title> ì¶”ì¶œ

- [x] **Test 2A.2**: SPA ê°ì§€ í…ŒìŠ¤íŠ¸
  - File(s): `tests/unit/crawler/test_page_analyzer.py` (ì¶”ê°€)
  - Expected: Tests FAIL (red)
  - Details:
    - `test_detect_spa_static_html_returns_false` â€” ì •ì  HTMLì€ SPA ì•„ë‹˜
    - `test_detect_spa_large_dom_diff_returns_true` â€” JS ë Œë”ë§ìœ¼ë¡œ DOMì´ í¬ê²Œ ë³€í•œ ê²½ìš° SPA
    - `test_detect_spa_react_root_marker_returns_true` â€” `<div id="root">` ë“± SPA í”„ë ˆì„ì›Œí¬ ë§ˆì»¤ ê°ì§€

**GREEN: Implement to Make Tests Pass**

- [x] **Task 2A.3**: PageAnalyzer í´ë˜ìŠ¤ êµ¬í˜„
  - File(s): `src/eazy/crawler/page_analyzer.py` (ì‹ ê·œ íŒŒì¼)
  - Goal: Test 2A.1 + Test 2A.2 í†µê³¼
  - Details:
    - `extract_links(page) -> list[str]` â€” Playwright ì…€ë ‰í„°ë¡œ <a> íƒœê·¸ ì¶”ì¶œ, ì ˆëŒ€ URL ë³€í™˜
    - `extract_forms(page) -> list[FormData]` â€” <form> íƒœê·¸ + <input> í•„ë“œ ì¶”ì¶œ
    - `extract_buttons(page) -> list[ButtonInfo]` â€” <button> íƒœê·¸ ì¶”ì¶œ
    - `extract_title(page) -> str | None` â€” <title> ì¶”ì¶œ
    - `detect_spa(page) -> bool` â€” SPA ê°ì§€ (DOM í¬ê¸° ë¹„êµ + í”„ë ˆì„ì›Œí¬ ë§ˆì»¤)
    - `analyze(page) -> PageAnalysisResult` â€” ìœ„ ë©”ì„œë“œë“¤ì„ í†µí•© ì‹¤í–‰
    - ê¸°ì¡´ `FormData`, `ButtonInfo` ëª¨ë¸ ì¬ì‚¬ìš©

**REFACTOR: Clean Up Code**

- [x] **Task 2A.4**: ì½”ë“œ í’ˆì§ˆ ê°œì„ 
  - Files: `src/eazy/crawler/page_analyzer.py`
  - Goal: í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ì„œ ì„¤ê³„ ê°œì„ 
  - Checklist:
    - [x] Google ìŠ¤íƒ€ì¼ docstring ì¶”ê°€
    - [x] ì…€ë ‰í„° ìƒìˆ˜ë¥¼ ëª¨ë“ˆ ë ˆë²¨ë¡œ ì¶”ì¶œ
    - [x] ì—ëŸ¬ ì²˜ë¦¬ (ì…€ë ‰í„° ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜)
    - [ ] REQ-002B í™•ì¥ í¬ì¸íŠ¸ ì£¼ì„ (LLM ì£¼ì… ê°€ëŠ¥ ìœ„ì¹˜)

#### Quality Gate

**STOP: Do NOT proceed to Phase 2B until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [x] **Red Phase**: Tests were written FIRST and initially failed
- [x] **Green Phase**: Production code written to make tests pass
- [x] **Refactor Phase**: Code improved while tests still pass
- [x] **Coverage Check**: PageAnalyzer ì»¤ë²„ë¦¬ì§€ 87% (>= 80%)

**Build & Tests**:
- [x] **All Tests Pass**: 228 existing + 12 new = 240 total passed
- [x] **No Flaky Tests**: ì¼ê´€ëœ ê²°ê³¼

**Code Quality**:
- [x] **Linting**: `uv run ruff check src/ tests/` â€” All checks passed
- [x] **Formatting**: `uv run ruff format --check src/ tests/` â€” Already formatted

**Validation Commands**:
```bash
uv run pytest tests/ -v
uv run pytest tests/ --cov=eazy.crawler.page_analyzer --cov-report=term-missing
uv run ruff check src/ tests/
uv run ruff format --check src/ tests/
```

**Manual Test Checklist**:
- [x] ì •ì  HTMLì—ì„œ ë§í¬/í¼/ë²„íŠ¼ ì •í™•íˆ ì¶”ì¶œ í™•ì¸
- [x] SPA(JS ë Œë”ë§ ì½˜í…ì¸ )ì—ì„œ ë™ì  ìš”ì†Œ ì¶”ì¶œ í™•ì¸
- [x] SPA ê°ì§€ ë¡œì§ì´ ì •ì /ë™ì  í˜ì´ì§€ë¥¼ ì˜¬ë°”ë¥´ê²Œ êµ¬ë¶„

---

### Phase 2B: Network Interceptor - API Endpoint Capture
**Goal**: Playwrightì˜ ë„¤íŠ¸ì›Œí¬ ì´ë²¤íŠ¸ë¥¼ ìº¡ì²˜í•˜ì—¬ XHR/fetch API í˜¸ì¶œì„ ì‹ë³„
**Estimated Time**: 2 hours
**Status**: âœ… Complete (248 tests, 100% coverage on NetworkInterceptor)

#### Tasks

**RED: Write Failing Tests First**

- [x] **Test 2B.1**: NetworkInterceptor ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
  - File(s): `tests/unit/crawler/test_network_interceptor.py` (ì‹ ê·œ íŒŒì¼)
  - Expected: Tests FAIL (red) because NetworkInterceptor doesn't exist
  - Details:
    - `test_capture_xhr_requests_returns_endpoint_info` â€” XHR ìš”ì²­ ìº¡ì²˜
    - `test_capture_fetch_requests_returns_endpoint_info` â€” fetch ìš”ì²­ ìº¡ì²˜
    - `test_capture_ignores_static_resources` â€” CSS, JS, ì´ë¯¸ì§€ ë“± ì •ì  ë¦¬ì†ŒìŠ¤ ë¬´ì‹œ
    - `test_capture_includes_request_method` â€” GET, POST ë“± ë©”ì„œë“œ í¬í•¨
    - `test_capture_includes_request_url` â€” ìš”ì²­ URL í¬í•¨
    - `test_capture_deduplicates_identical_api_calls` â€” ë™ì¼ API í˜¸ì¶œ ì¤‘ë³µ ì œê±°
    - `test_start_capture_before_navigation_captures_initial_requests` â€” ë„¤ë¹„ê²Œì´ì…˜ ì „ ì‹œì‘í•˜ë©´ ì´ˆê¸° ìš”ì²­ë„ ìº¡ì²˜
    - `test_stop_capture_returns_collected_endpoints` â€” ìº¡ì²˜ ì¤‘ì§€ í›„ ê²°ê³¼ ë°˜í™˜

**GREEN: Implement to Make Tests Pass**

- [x] **Task 2B.2**: NetworkInterceptor í´ë˜ìŠ¤ êµ¬í˜„
  - File(s): `src/eazy/crawler/network_interceptor.py` (ì‹ ê·œ íŒŒì¼)
  - Goal: Test 2B.1 í†µê³¼
  - Details:
    - `start(page)` â€” `page.on("request", handler)` ë“±ë¡
    - `stop()` â€” listener ì œê±° + ê²°ê³¼ ë°˜í™˜
    - `get_endpoints() -> list[EndpointInfo]` â€” ìº¡ì²˜ëœ API ì—”ë“œí¬ì¸íŠ¸ ë°˜í™˜
    - í•„í„°ë§: `resource_type`ì´ `xhr` ë˜ëŠ” `fetch`ì¸ ìš”ì²­ë§Œ ìº¡ì²˜
    - ì •ì  ë¦¬ì†ŒìŠ¤ ë¬´ì‹œ: `stylesheet`, `image`, `font`, `script` íƒ€ì… ì œì™¸
    - ê¸°ì¡´ `EndpointInfo` ëª¨ë¸ ì¬ì‚¬ìš©

**REFACTOR: Clean Up Code**

- [x] **Task 2B.3**: ì½”ë“œ í’ˆì§ˆ ê°œì„ 
  - Files: `src/eazy/crawler/network_interceptor.py`
  - Goal: í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ì„œ ì„¤ê³„ ê°œì„ 
  - Checklist:
    - [x] Google ìŠ¤íƒ€ì¼ docstring ì¶”ê°€
    - [x] listener cleanup ë³´ì¥ (stop()ì—ì„œ remove_listener í˜¸ì¶œ)
    - [x] ì¤‘ë³µ ì œê±° ë¡œì§ ìµœì í™” (set[tuple[str, str]] + frozenset í•„í„°)

#### Quality Gate

**STOP: Do NOT proceed to Phase 3 until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [x] **Red Phase**: Tests were written FIRST and initially failed
- [x] **Green Phase**: Production code written to make tests pass
- [x] **Refactor Phase**: Code improved while tests still pass
- [x] **Coverage Check**: NetworkInterceptor ì»¤ë²„ë¦¬ì§€ 100% (>= 80%)

**Build & Tests**:
- [x] **All Tests Pass**: 240 existing + 8 new = 248 total passed
- [x] **No Flaky Tests**: ì¼ê´€ëœ ê²°ê³¼

**Code Quality**:
- [x] **Linting**: `uv run ruff check src/ tests/` â€” All checks passed
- [x] **Formatting**: `uv run ruff format --check src/ tests/` â€” Already formatted

**Validation Commands**:
```bash
uv run pytest tests/ -v
uv run pytest tests/ --cov=eazy.crawler.network_interceptor --cov-report=term-missing
uv run ruff check src/ tests/
uv run ruff format --check src/ tests/
```

**Manual Test Checklist**:
- [x] XHR ìš”ì²­ì´ EndpointInfoë¡œ ì •í™•íˆ ìº¡ì²˜ë¨
- [x] fetch ìš”ì²­ì´ EndpointInfoë¡œ ì •í™•íˆ ìº¡ì²˜ë¨
- [x] ì´ë¯¸ì§€/CSS/í°íŠ¸ ë“± ì •ì  ë¦¬ì†ŒìŠ¤ëŠ” ë¬´ì‹œë¨

---

### Phase 3: Smart Crawler Engine - Core Crawling Logic
**Goal**: SmartCrawlerEngine í´ë˜ìŠ¤ â€” Playwright ê¸°ë°˜ BFS í¬ë¡¤ë§, ê¸°ì¡´ ëª¨ë“ˆ ì¬ì‚¬ìš©
**Estimated Time**: 4 hours
**Status**: âœ… Complete

#### Tasks

**RED: Write Failing Tests First**

- [x] **Test 3.1**: SmartCrawlerEngine ê¸°ë³¸ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
  - File(s): `tests/integration/crawler/test_smart_engine.py` (ì‹ ê·œ íŒŒì¼)
  - Expected: Tests FAIL (red) because SmartCrawlerEngine doesn't exist
  - Details:
    - `test_crawl_single_page_returns_page_result` â€” ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§
    - `test_crawl_follows_links_to_next_page` â€” ë§í¬ ë”°ë¼ê°€ê¸°
    - `test_crawl_extracts_forms_and_buttons` â€” í¼/ë²„íŠ¼ ì¶”ì¶œ
    - `test_crawl_captures_api_endpoints` â€” API ì—”ë“œí¬ì¸íŠ¸ ìº¡ì²˜
    - `test_crawl_spa_javascript_rendered_content` â€” SPA ì½˜í…ì¸  í¬ë¡¤ë§
    - `test_crawl_returns_crawl_result_with_statistics` â€” CrawlResult ë°˜í™˜

- [x] **Test 3.2**: ì„¤ì • ë° ì œì•½ ì¡°ê±´ í…ŒìŠ¤íŠ¸
  - File(s): `tests/integration/crawler/test_smart_engine.py` (ì¶”ê°€)
  - Expected: Tests FAIL (red)
  - Details:
    - `test_crawl_respects_max_depth` â€” ê¹Šì´ ì œí•œ
    - `test_crawl_respects_max_pages` â€” í˜ì´ì§€ ìˆ˜ ì œí•œ
    - `test_crawl_respects_robots_txt` â€” robots.txt ì¤€ìˆ˜
    - `test_crawl_enforces_scope` â€” ì™¸ë¶€ ë„ë©”ì¸ ì°¨ë‹¨
    - `test_crawl_handles_navigation_timeout` â€” íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
    - `test_crawl_handles_page_error` â€” í˜ì´ì§€ ì—ëŸ¬ ì²˜ë¦¬ (4xx, 5xx)
    - `test_crawl_deduplicates_with_url_pattern_normalizer` â€” URL íŒ¨í„´ ì •ê·œí™” ì¬ì‚¬ìš©
    - `test_crawl_excludes_urls_matching_exclude_patterns` â€” ì œì™¸ íŒ¨í„´

**GREEN: Implement to Make Tests Pass**

- [x] **Task 3.3**: SmartCrawlerEngine í´ë˜ìŠ¤ êµ¬í˜„
  - File(s): `src/eazy/crawler/smart_engine.py` (ì‹ ê·œ íŒŒì¼)
  - Goal: Test 3.1 + Test 3.2 í†µê³¼
  - Details:
    - `__init__(config: CrawlConfig)` â€” ì„¤ì • + BrowserManager + PageAnalyzer + NetworkInterceptor ì´ˆê¸°í™”
    - `async crawl() -> CrawlResult` â€” BFS í¬ë¡¤ë§ ì‹¤í–‰
    - BFS deque ê¸°ë°˜ (url, depth, parent_url) í
    - ê° í˜ì´ì§€: BrowserManager.new_page() â†’ NetworkInterceptor.start() â†’ page.goto() â†’ PageAnalyzer.analyze() â†’ NetworkInterceptor.stop()
    - ê¸°ì¡´ ëª¨ë“ˆ ì¬ì‚¬ìš©: `url_resolver.resolve_url()`, `url_resolver.normalize_url()`, `url_resolver.is_in_scope()`, `RobotsParser`, `URLPatternNormalizer`, `Sitemap`
    - CrawlResult êµ¬ì„± with statistics

- [x] **Task 3.4**: ê¸°ì¡´ ëª¨ë“ˆ í†µí•© ê²€ì¦
  - File(s): `src/eazy/crawler/smart_engine.py`
  - Goal: ê¸°ì¡´ url_resolver, robots_parser, url_pattern ëª¨ë“ˆê³¼ ì •ìƒ ì—°ë™ í™•ì¸
  - Details:
    - robots.txtëŠ” httpxë¡œ ì§ì ‘ fetch (Playwright ë¶ˆí•„ìš”)
    - URL ì •ê·œí™”/ìŠ¤ì½”í”„ ì²´í¬ëŠ” ê¸°ì¡´ í•¨ìˆ˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - íŒ¨í„´ ì •ê·œí™”ëŠ” ê¸°ì¡´ URLPatternNormalizer ì¬ì‚¬ìš©

**REFACTOR: Clean Up Code**

- [x] **Task 3.5**: ì½”ë“œ í’ˆì§ˆ ê°œì„ 
  - Files: `src/eazy/crawler/smart_engine.py`
  - Goal: í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ì„œ ì„¤ê³„ ê°œì„ 
  - Checklist:
    - [x] Google ìŠ¤íƒ€ì¼ docstring ì¶”ê°€
    - [x] BFS ë¡œì§ ê°€ë…ì„± ê°œì„ 
    - [x] ì—ëŸ¬ ì²˜ë¦¬ í†µí•© (navigation error, timeout, connection error)
    - [x] ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ë³´ì¥ (ëª¨ë“  ê²½ë¡œì—ì„œ browser close)

#### Quality Gate

**STOP: Do NOT proceed to Phase 4 until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [x] **Red Phase**: Tests were written FIRST and initially failed
- [x] **Green Phase**: Production code written to make tests pass
- [x] **Refactor Phase**: Code improved while tests still pass
- [x] **Coverage Check**: SmartCrawlerEngine ì»¤ë²„ë¦¬ì§€ 96% (>= 80%)

**Build & Tests**:
- [x] **All Tests Pass**: 262 tests passed (248 existing + 14 new)
- [x] **No Flaky Tests**: ì¼ê´€ëœ ê²°ê³¼

**Code Quality**:
- [x] **Linting**: `uv run ruff check src/ tests/` â€” ì—ëŸ¬ ì—†ìŒ
- [x] **Formatting**: `uv run ruff format --check src/ tests/` â€” ë³€ê²½ ì—†ìŒ
- [x] **Type Safety**: ëª¨ë“  í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ ì ìš©

**Security & Performance**:
- [x] **Resource Cleanup**: ëª¨ë“  ê²½ë¡œì—ì„œ ë¸Œë¼ìš°ì €/í˜ì´ì§€ ì •ìƒ ì¢…ë£Œ (try/finally)
- [x] **Memory**: í˜ì´ì§€ ë°©ë¬¸ í›„ ì¦‰ì‹œ ë‹«ê¸°ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬

**Validation Commands**:
```bash
uv run pytest tests/ -v
uv run pytest tests/ --cov=src/eazy/crawler/smart_engine --cov-report=term-missing
uv run ruff check src/ tests/
uv run ruff format --check src/ tests/
```

**Manual Test Checklist** (automated tests cover all):
- [x] ë‹¨ì¼ ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ â†’ PageResult ë°˜í™˜ (`test_crawl_single_page_returns_page_result`)
- [x] ë§í¬ê°€ ìˆëŠ” ì‚¬ì´íŠ¸ í¬ë¡¤ë§ â†’ ì—¬ëŸ¬ í˜ì´ì§€ íƒìƒ‰ (`test_crawl_follows_links_to_next_page`)
- [x] SPA ì‚¬ì´íŠ¸ í¬ë¡¤ë§ â†’ JS ë Œë”ë§ ì½˜í…ì¸  ì¶”ì¶œ (`test_crawl_spa_javascript_rendered_content`)
- [x] max_depth=0 â†’ ë£¨íŠ¸ í˜ì´ì§€ë§Œ í¬ë¡¤ë§ (`test_crawl_respects_max_depth`)
- [x] robots.txt ì°¨ë‹¨ URL â†’ ìŠ¤í‚µ (`test_crawl_respects_robots_txt`)

---

### Phase 4: Knowledge Graph & Export
**Goal**: KnowledgeGraph ëª¨ë¸, GraphBuilder ë³€í™˜ê¸°, JSON ë‚´ë³´ë‚´ê¸°
**Estimated Time**: 3 hours
**Status**: Pending

#### Tasks

**RED: Write Failing Tests First**

- [ ] **Test 4.1**: Knowledge Graph ëª¨ë¸ í…ŒìŠ¤íŠ¸
  - File(s): `tests/unit/models/test_crawl_types.py` (ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€)
  - Expected: Tests FAIL (red) because graph models don't exist
  - Details:
    - `test_graph_node_type_has_page_api_resource` â€” GraphNodeType enum ê°’
    - `test_graph_edge_type_has_hyperlink_form_api_redirect` â€” GraphEdgeType enum ê°’
    - `test_graph_node_creation` â€” GraphNode ìƒì„± (url, type, metadata)
    - `test_graph_node_frozen_immutable` â€” frozen ëª¨ë¸
    - `test_graph_edge_creation` â€” GraphEdge ìƒì„± (source, target, type)
    - `test_knowledge_graph_creation` â€” KnowledgeGraph ë¹ˆ ìƒì„±
    - `test_knowledge_graph_add_node` â€” ë…¸ë“œ ì¶”ê°€
    - `test_knowledge_graph_add_edge` â€” ì—£ì§€ ì¶”ê°€
    - `test_knowledge_graph_get_nodes_by_type` â€” íƒ€ì…ë³„ ë…¸ë“œ ì¡°íšŒ
    - `test_knowledge_graph_statistics` â€” ë…¸ë“œ/ì—£ì§€ ìˆ˜ í†µê³„

- [ ] **Test 4.2**: GraphBuilder ë³€í™˜ í…ŒìŠ¤íŠ¸
  - File(s): `tests/unit/crawler/test_graph_builder.py` (ì‹ ê·œ íŒŒì¼)
  - Expected: Tests FAIL (red) because GraphBuilder doesn't exist
  - Details:
    - `test_build_graph_from_crawl_result_creates_page_nodes` â€” í˜ì´ì§€ë‹¹ ë…¸ë“œ ìƒì„±
    - `test_build_graph_includes_hyperlink_edges` â€” ë§í¬ â†’ hyperlink ì—£ì§€
    - `test_build_graph_includes_form_action_edges` â€” í¼ â†’ form_action ì—£ì§€
    - `test_build_graph_includes_api_call_edges` â€” API â†’ api_call ì—£ì§€
    - `test_build_graph_deduplicates_nodes_by_url` â€” URL ì¤‘ë³µ ë…¸ë“œ ì œê±°
    - `test_build_graph_empty_crawl_result_returns_empty_graph` â€” ë¹ˆ ê²°ê³¼ ì²˜ë¦¬
    - `test_export_graph_to_json_valid_format` â€” JSON ì§ë ¬í™”

**GREEN: Implement to Make Tests Pass**

- [ ] **Task 4.3**: Knowledge Graph ëª¨ë¸ ì¶”ê°€
  - File(s): `src/eazy/models/crawl_types.py`
  - Goal: Test 4.1 í†µê³¼
  - Details:
    - `GraphNodeType(str, Enum)` â€” page, api, resource
    - `GraphEdgeType(str, Enum)` â€” hyperlink, form_action, api_call, redirect
    - `GraphNode(BaseModel, frozen=True)` â€” url, node_type, metadata (dict)
    - `GraphEdge(BaseModel, frozen=True)` â€” source, target, edge_type, metadata (dict)
    - `KnowledgeGraph(BaseModel)` â€” nodes (dict[str, GraphNode]), edges (list[GraphEdge])
    - `add_node()`, `add_edge()`, `get_nodes_by_type()`, `statistics` property

- [ ] **Task 4.4**: GraphBuilder í´ë˜ìŠ¤ êµ¬í˜„
  - File(s): `src/eazy/crawler/graph_builder.py` (ì‹ ê·œ íŒŒì¼)
  - Goal: Test 4.2 í†µê³¼
  - Details:
    - `build(crawl_result: CrawlResult) -> KnowledgeGraph` â€” ë³€í™˜ ë¡œì§
    - ê° PageResult â†’ GraphNode(type=PAGE)
    - ê° link â†’ GraphEdge(type=HYPERLINK)
    - ê° form.action â†’ GraphEdge(type=FORM_ACTION)
    - ê° api_endpoint â†’ GraphNode(type=API) + GraphEdge(type=API_CALL)
    - URL ì •ê·œí™”ë¡œ ì¤‘ë³µ ë…¸ë“œ ì œê±°
    - `to_json(graph: KnowledgeGraph) -> str` â€” JSON ë‚´ë³´ë‚´ê¸°

**REFACTOR: Clean Up Code**

- [ ] **Task 4.5**: ì½”ë“œ í’ˆì§ˆ ê°œì„ 
  - Files: `src/eazy/models/crawl_types.py`, `src/eazy/crawler/graph_builder.py`
  - Goal: í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ì„œ ì„¤ê³„ ê°œì„ 
  - Checklist:
    - [ ] Google ìŠ¤íƒ€ì¼ docstring ì¶”ê°€
    - [ ] `__all__` export ë¦¬ìŠ¤íŠ¸ ì •ë¦¬ (models/__init__.py ì—…ë°ì´íŠ¸)
    - [ ] KnowledgeGraph ë©”ì„œë“œ ê°€ë…ì„± ê°œì„ 

#### Quality Gate

**STOP: Do NOT proceed to Phase 5 until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [ ] **Red Phase**: Tests were written FIRST and initially failed
- [ ] **Green Phase**: Production code written to make tests pass
- [ ] **Refactor Phase**: Code improved while tests still pass
- [ ] **Coverage Check**: GraphBuilder ì»¤ë²„ë¦¬ì§€ >= 80%, crawl_types.py 100%

**Build & Tests**:
- [ ] **All Tests Pass**: ê¸°ì¡´ + Phase 1~4 í…ŒìŠ¤íŠ¸ ì „ë¶€ í†µê³¼
- [ ] **No Flaky Tests**: ì¼ê´€ëœ ê²°ê³¼

**Code Quality**:
- [ ] **Linting**: `uv run ruff check src/ tests/` â€” ì—ëŸ¬ ì—†ìŒ
- [ ] **Formatting**: `uv run ruff format --check src/ tests/` â€” ë³€ê²½ ì—†ìŒ

**Validation Commands**:
```bash
uv run pytest tests/ -v
uv run pytest tests/ --cov=src/eazy --cov-report=term-missing
uv run ruff check src/ tests/
uv run ruff format --check src/ tests/
```

**Manual Test Checklist**:
- [ ] CrawlResult â†’ KnowledgeGraph ë³€í™˜ í›„ ë…¸ë“œ/ì—£ì§€ ìˆ˜ ì •í™•
- [ ] JSON ì¶œë ¥ì— nodes, edges í¬í•¨
- [ ] ì¤‘ë³µ ë…¸ë“œê°€ ì œê±°ë¨

---

### Phase 5: Integration & CLI
**Goal**: CLIì— `--smart` ì˜µì…˜ ì¶”ê°€, ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸, CrawlResultì— KnowledgeGraph í¬í•¨
**Estimated Time**: 3 hours
**Status**: Pending

#### Tasks

**RED: Write Failing Tests First**

- [ ] **Test 5.1**: CLI í†µí•© í…ŒìŠ¤íŠ¸
  - File(s): `tests/unit/cli/test_crawl_command.py` (ê¸°ì¡´ íŒŒì¼ì— ì¶”ê°€)
  - Expected: Tests FAIL (red) because --smart option doesn't exist
  - Details:
    - `test_crawl_command_smart_option_exists` â€” --smart ì˜µì…˜ ì¸ì‹
    - `test_crawl_command_smart_invokes_smart_engine` â€” --smart ì‹œ SmartCrawlerEngine ì‚¬ìš©
    - `test_crawl_command_without_smart_uses_regex_engine` â€” ê¸°ë³¸ì€ ê¸°ì¡´ CrawlerEngine
    - `test_crawl_command_smart_output_json_includes_graph` â€” JSON ì¶œë ¥ì— knowledge_graph í¬í•¨

- [ ] **Test 5.2**: ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸
  - File(s): `tests/integration/crawler/test_smart_engine.py` (ì¶”ê°€)
  - Expected: Tests FAIL (red)
  - Details:
    - `test_smart_crawl_end_to_end_workflow` â€” í¬ë¡¤ë§ â†’ ê·¸ë˜í”„ ë¹Œë“œ â†’ JSON ë‚´ë³´ë‚´ê¸° ì „ì²´
    - `test_smart_crawl_result_includes_knowledge_graph` â€” CrawlResultì— KnowledgeGraph í¬í•¨
    - `test_smart_crawl_with_pattern_normalization` â€” URL íŒ¨í„´ ì •ê·œí™” ì—°ë™

**GREEN: Implement to Make Tests Pass**

- [ ] **Task 5.3**: CLI ëª…ë ¹ì–´ í™•ì¥
  - File(s): `src/eazy/cli/` (ê¸°ì¡´ CLI íŒŒì¼ ìˆ˜ì •)
  - Goal: Test 5.1 í†µê³¼
  - Details:
    - `eazy crawl` ëª…ë ¹ì— `--smart` ì˜µì…˜ ì¶”ê°€
    - `--smart` ì‹œ SmartCrawlerEngine ì‚¬ìš©, ì•„ë‹ˆë©´ ê¸°ì¡´ CrawlerEngine
    - ê²°ê³¼ì— KnowledgeGraph í¬í•¨ ì‹œ JSONì— `knowledge_graph` í•„ë“œ ì¶”ê°€

- [ ] **Task 5.4**: CrawlResultì— KnowledgeGraph í†µí•©
  - File(s): `src/eazy/models/crawl_types.py`, `src/eazy/crawler/smart_engine.py`
  - Goal: Test 5.2 í†µê³¼
  - Details:
    - CrawlResultì— `knowledge_graph: KnowledgeGraph | None = None` í•„ë“œ ì¶”ê°€
    - SmartCrawlerEngine.crawl()ì—ì„œ GraphBuilderë¡œ ê·¸ë˜í”„ ìƒì„± í›„ CrawlResultì— í¬í•¨
    - ê¸°ì¡´ exporterê°€ ìë™ ì§ë ¬í™” (Pydantic model_dump)

**REFACTOR: Clean Up Code**

- [ ] **Task 5.5**: ìµœì¢… ì½”ë“œ í’ˆì§ˆ ê°œì„ 
  - Files: ì „ì²´ ì‹ ê·œ íŒŒì¼
  - Goal: í…ŒìŠ¤íŠ¸ ê¹¨ì§€ì§€ ì•Šìœ¼ë©´ì„œ ìµœì¢… ì •ë¦¬
  - Checklist:
    - [ ] ëª¨ë“  `__init__.py` export ì •ë¦¬
    - [ ] ë¶ˆí•„ìš”í•œ import ì œê±°
    - [ ] ì „ì²´ ì½”ë“œ ë¦°íŒ…/í¬ë§·íŒ… ìµœì¢… í™•ì¸
    - [ ] ê¸°ì¡´ 215ê°œ í…ŒìŠ¤íŠ¸ + ëª¨ë“  ì‹ ê·œ í…ŒìŠ¤íŠ¸ ì „ë¶€ í†µê³¼

#### Quality Gate

**STOP: Do NOT mark complete until ALL checks pass**

**TDD Compliance** (CRITICAL):
- [ ] **Red Phase**: Tests were written FIRST and initially failed
- [ ] **Green Phase**: Production code written to make tests pass
- [ ] **Refactor Phase**: Code improved while tests still pass
- [ ] **Coverage Check**: ì „ì²´ ì»¤ë²„ë¦¬ì§€ >= 80%

**Build & Tests**:
- [ ] **Build**: í”„ë¡œì íŠ¸ ì—ëŸ¬ ì—†ì´ ë¹Œë“œ
- [ ] **All Tests Pass**: ê¸°ì¡´ 215ê°œ + ëª¨ë“  ì‹ ê·œ í…ŒìŠ¤íŠ¸ ì „ë¶€ í†µê³¼
- [ ] **No Flaky Tests**: ì¼ê´€ëœ ê²°ê³¼ (3íšŒ ì´ìƒ ì‹¤í–‰)

**Code Quality**:
- [ ] **Linting**: `uv run ruff check src/ tests/` â€” ì—ëŸ¬ ì—†ìŒ
- [ ] **Formatting**: `uv run ruff format --check src/ tests/` â€” ë³€ê²½ ì—†ìŒ
- [ ] **Type Safety**: ëª¨ë“  ìƒˆ í•¨ìˆ˜ì— íƒ€ì… íŒíŠ¸ ì ìš©

**Security & Performance**:
- [ ] **Dependencies**: `playwright` ë³´ì•ˆ ì·¨ì•½ì  ì—†ìŒ í™•ì¸
- [ ] **Resource Cleanup**: ëª¨ë“  ê²½ë¡œì—ì„œ ë¸Œë¼ìš°ì € ì •ìƒ ì¢…ë£Œ
- [ ] **Memory**: í˜ì´ì§€ ë°©ë¬¸ í›„ ì¦‰ì‹œ ë‹«ê¸°

**Documentation**:
- [ ] **Code Comments**: ë³µì¡í•œ ë¡œì§ì— ì¸ë¼ì¸ ì£¼ì„
- [ ] **Docstring**: ëª¨ë“  public í•¨ìˆ˜ì— Google ìŠ¤íƒ€ì¼ docstring

**Validation Commands**:
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
uv run pytest tests/ -v

# ì „ì²´ ì»¤ë²„ë¦¬ì§€
uv run pytest tests/ --cov=src/eazy --cov-report=term-missing

# ë¦°íŒ…/í¬ë§·íŒ…
uv run ruff check src/ tests/
uv run ruff format --check src/ tests/
```

**Manual Test Checklist**:
- [ ] `eazy crawl --smart https://example.com` ì •ìƒ ë™ì‘
- [ ] `eazy crawl https://example.com` (ê¸°ì¡´) ì •ìƒ ë™ì‘ (regression ì—†ìŒ)
- [ ] JSON ì¶œë ¥ì— knowledge_graph í•„ë“œ í¬í•¨

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation Strategy |
|------|-------------|--------|---------------------|
| Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ CI ì´ìŠˆ | Medium | High | CIì— `playwright install --with-deps chromium` ì¶”ê°€. ë¡œì»¬ ê°€ì´ë“œ ì œê³µ |
| SPA ì‚¬ì´íŠ¸ë³„ ë¡œë”© ì™„ë£Œ ê°ì§€ ì–´ë ¤ì›€ | High | Medium | configurable wait strategy + SPA ìë™ ê°ì§€ ì¡°í•©. networkidle ê¸°ë³¸ê°’ |
| ê¸°ì¡´ 215 í…ŒìŠ¤íŠ¸ ê¹¨ì§ | Low | High | ê¸°ì¡´ CrawlConfigì— ê¸°ë³¸ê°’ í•„ë“œë§Œ ì¶”ê°€. backward compatible |
| Playwright í…ŒìŠ¤íŠ¸ flakiness | Medium | Medium | page.route() ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ mock + proper cleanup fixture |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ (headless browser) | Medium | Medium | ë‹¨ì¼ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤, í˜ì´ì§€ ì¦‰ì‹œ ë‹«ê¸°, max_pages ì œí•œ |
| ì§€ì‹ ê·¸ë˜í”„ ëŒ€ê·œëª¨ ì‚¬ì´íŠ¸ ì„±ëŠ¥ | Low | Medium | in-memory Pydantic â†’ í–¥í›„ NetworkX/DBë¡œ í™•ì¥ ê°€ëŠ¥ |

---

## Rollback Strategy

### If Phase 1 Fails
**Steps to revert**:
- `pyproject.toml`ì—ì„œ playwright ì˜ì¡´ì„± ì œê±°
- `src/eazy/crawler/browser_manager.py` ì‚­ì œ
- `src/eazy/models/crawl_types.py`ì—ì„œ ì¶”ê°€í•œ í•„ë“œ ì œê±°
- `tests/unit/crawler/test_browser_manager.py` ì‚­ì œ

### If Phase 2A/2B Fails
**Steps to revert**:
- Phase 1 ì™„ë£Œ ìƒíƒœë¡œ ë³µì›
- `src/eazy/crawler/page_analyzer.py` ì‚­ì œ
- `src/eazy/crawler/network_interceptor.py` ì‚­ì œ
- ê´€ë ¨ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‚­ì œ

### If Phase 3 Fails
**Steps to revert**:
- Phase 2B ì™„ë£Œ ìƒíƒœë¡œ ë³µì›
- `src/eazy/crawler/smart_engine.py` ì‚­ì œ
- `tests/integration/crawler/test_smart_engine.py` ì‚­ì œ

### If Phase 4 Fails
**Steps to revert**:
- Phase 3 ì™„ë£Œ ìƒíƒœë¡œ ë³µì›
- `src/eazy/crawler/graph_builder.py` ì‚­ì œ
- `src/eazy/models/crawl_types.py`ì—ì„œ Graph ëª¨ë¸ ì œê±°
- ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì œê±°

### If Phase 5 Fails
**Steps to revert**:
- Phase 4 ì™„ë£Œ ìƒíƒœë¡œ ë³µì›
- CLI ë³€ê²½ ë³µì›
- CrawlResultì—ì„œ knowledge_graph í•„ë“œ ì œê±°
- í†µí•© í…ŒìŠ¤íŠ¸ ì œê±°

---

## Progress Tracking

### Completion Status
- **Phase 1**: 100% âœ…
- **Phase 2A**: 100% âœ…
- **Phase 2B**: 100% âœ…
- **Phase 3**: 100% âœ…
- **Phase 4**: 0%
- **Phase 5**: 0%

**Overall Progress**: 67% complete (4/6 phases)

### Time Tracking
| Phase | Estimated | Actual | Variance |
|-------|-----------|--------|----------|
| Phase 1 | 3 hours | ~30 min | -2.5h (faster) |
| Phase 2A | 2 hours | ~15 min | -1.75h (faster) |
| Phase 2B | 2 hours | ~10 min | -1.83h (faster) |
| Phase 3 | 4 hours | ~15 min | -3.75h (faster) |
| Phase 4 | 3 hours | - | - |
| Phase 5 | 3 hours | - | - |
| **Total** | **17 hours** | - | - |

---

## Notes & Learnings

### Implementation Notes
- Phase 1: Playwright mock with `unittest.mock.AsyncMock` + `@patch` â€” conftest fixture ë¶ˆí•„ìš”
- Phase 1: `TYPE_CHECKING` guardë¡œ Playwright import overhead ìµœì†Œí™”
- Phase 1: BrowserManager.close()ëŠ” idempotent (ì¤‘ë³µ í˜¸ì¶œ ì•ˆì „)
- Phase 2A: `_make_element(**attrs)` helperë¡œ Playwright element mock íŒ¨í„´í™”
- Phase 2A: `resolve_url()` ì¬ì‚¬ìš©ìœ¼ë¡œ URL ë³€í™˜ ì¤‘ë³µ ì œê±°
- Phase 2A: SPA ê°ì§€ = í”„ë ˆì„ì›Œí¬ ë§ˆì»¤(#root, #app ë“±) + script count threshold(>=5)
- Phase 2A: ëª¨ë“  selector í˜¸ì¶œì„ try/exceptë¡œ ê°ì‹¸ì„œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (87% coverage, ë¯¸ì»¤ë²„ ë¼ì¸ì€ except ë¶„ê¸°)
- Phase 2B: `page.on()` / `page.remove_listener()` are sync â€” no async/await needed
- Phase 2B: MagicMock (not AsyncMock) for request handler tests, no `pytest.mark.asyncio`
- Phase 2B: `_API_RESOURCE_TYPES` as `ClassVar[frozenset[str]]` for O(1) membership check
- Phase 2B: Dedup via `set[tuple[str, str]]` keyed on (url, method) â€” simple and effective

### Blockers Encountered
- (Phase ì§„í–‰ ì‹œ ì¶”ê°€)

### Improvements for Future Plans
- (Phase ì§„í–‰ ì‹œ ì¶”ê°€)

---

## References

### Documentation
- PRD REQ-002A ìŠ¤í™: `plan/PRD.md` (lines 115-129)
- Playwright Python docs: https://playwright.dev/python/
- ê¸°ì¡´ í¬ë¡¤ë§ ì—”ì§„: `src/eazy/crawler/engine.py`
- ê¸°ì¡´ ë°ì´í„° ëª¨ë¸: `src/eazy/models/crawl_types.py`

### Related Issues
- Branch: `feature/req-002a-smart-crawling`
- ì„ í–‰ ì‘ì—…: REQ-001 ì „ì²´ ì™„ë£Œ (feature/req-001-* ë¸Œëœì¹˜ë“¤)

---

## Final Checklist

**Before marking plan as COMPLETE**:
- [ ] All phases completed with quality gates passed
- [ ] Full integration testing performed
- [ ] ê¸°ì¡´ 215ê°œ í…ŒìŠ¤íŠ¸ + ëª¨ë“  ì‹ ê·œ í…ŒìŠ¤íŠ¸ ì „ë¶€ í†µê³¼
- [ ] ì „ì²´ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ
- [ ] PRD REQ-002A 6ê°œ AC ëª¨ë‘ ì²´í¬ ì™„ë£Œ
- [ ] Plan document archived for future reference
